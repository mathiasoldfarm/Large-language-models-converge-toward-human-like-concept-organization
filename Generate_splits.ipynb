{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyOVoLo6oKEQ7v75U3w63+h+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install easydict\n","!pip install ijson"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I_a9ogLve6Kt","executionInfo":{"status":"ok","timestamp":1680268354107,"user_tz":-120,"elapsed":8944,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"}},"outputId":"b45d8651-6126-484a-96bf-1c837cf4a151"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: easydict in /usr/local/lib/python3.9/dist-packages (1.10)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ijson\n","  Downloading ijson-3.2.0.post0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (112 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 KB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: ijson\n","Successfully installed ijson-3.2.0.post0\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from google.colab import drive\n","from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression\n","import gensim.downloader\n","import os\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n","from sklearn.preprocessing import normalize\n","from sklearn import ensemble\n","import matplotlib.pyplot as plt\n","import random\n","import torch\n","import torch.nn as nn\n","from tqdm import tqdm\n","import math\n","import sys\n","import nltk\n","import pickle\n","import easydict\n","import requests\n","import ijson\n","import re"],"metadata":{"id":"ZIEAZPnrRAk3","executionInfo":{"status":"ok","timestamp":1680268357207,"user_tz":-120,"elapsed":3105,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["drive.mount('/content/drive', force_remount=True)\n","path = \"/content/drive/MyDrive/Master Thesis/\"\n","reference_space_path = \"/content/drive/MyDrive/Master Thesis/Reference spaces/\"\n","splits_path = path+\"splits/\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rb6Qc2pGQ3jf","executionInfo":{"status":"ok","timestamp":1680268381734,"user_tz":-120,"elapsed":24535,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"}},"outputId":"70b738dc-badc-4b16-abc7-ac0762f466ff"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":8,"metadata":{"id":"7ujFMuVJQhbb","executionInfo":{"status":"ok","timestamp":1680268381734,"user_tz":-120,"elapsed":3,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"}}},"outputs":[],"source":["def train_test_split(name, test_size):\n","  train_size = 1 - test_size\n","  with open(splits_path+f\"{name}_keys.txt\", 'r', encoding=\"utf-8\") as f:\n","        lines = f.readlines()\n","  test_n = int(len(lines) * test_size)\n","  indicies = set(random.sample(range(len(lines)), test_n))\n","  train = []\n","  test = []\n","  for i, line in enumerate(lines):\n","      if i in indicies: test.append(line)\n","      else: train.append(line)\n","  with open(splits_path+f\"train_{name}.txt\", 'w') as f:\n","    f.writelines(train)\n","\n","  with open(splits_path+f\"test_{name}.txt\", 'w') as f:\n","    f.writelines(test)\n","\n","def load_keys(filename, split=False, nltk_filter=None):\n","    with open(splits_path+filename if split else path+filename, \"r\") as f:\n","        lines = f.readlines()\n","    #random.shuffle(lines)\n","    keys = list(map(lambda line: line.replace(\"\\n\", \"\").lower(), lines))\n","    if nltk_filter is not None:\n","      nltk.download('averaged_perceptron_tagger')\n","      filtered_keys = []\n","      for key in keys:\n","        tag = nltk.pos_tag([key])\n","        if tag[0][1] in nltk_filter:\n","          filtered_keys.append(tag[0][0])\n","      return filtered_keys\n","    return set(keys)"]},{"cell_type":"markdown","source":["#Biggraph"],"metadata":{"id":"mwWKFPkYRRbX"}},{"cell_type":"code","source":["def load_biggraph(keysnames):\n","  name = \"biggraph\"  \n","  for keysname in keysnames:\n","    if os.path.exists(reference_space_path+f\"{name}_{keysname}_reference_space.npy\"):\n","      print(reference_space_path+f\"{name}_{keysname}_reference_space.npy\" + \" exists.\")\n","      continue\n","    print(\"Loading model\")\n","    f = open(path+\"wikidata_translation_v1_names.json\", 'r', encoding=\"utf-8\")\n","    objs = ijson.items(f, 'item')\n","\n","    biggraph = np.load(path+'wikidata_translation_v1_vectors.npy', mmap_mode='r')\n","\n","    all_keys = load_keys(f\"biggraph_{keysname}_keys.txt\", split=True)\n","    print(len(all_keys))\n","    \n","    aliasses = []\n","    idx = []\n","    print(\"Making reference space\")\n","    for i, o in enumerate(objs):\n","      word = o.lower().replace(\"\\\"\", \"\")\n","      word = re.sub(\"\\@(.*)\", \"\", word)\n","      if word in all_keys and word not in aliasses:\n","        aliasses.append(word)\n","        idx.append(i)\n","    reference_space = biggraph[idx]\n","    print(f\"Size: {len(aliasses)}\")\n","\n","    reference_space = np.append(reference_space, np.array(aliasses).reshape(len(aliasses),1), axis=1)\n","    print(\"Writing file\")\n","    with open(reference_space_path+f\"{name}_{keysname}_reference_space.npy\", \"wb\") as f:\n","        np.save(f, reference_space)\n","\n","    s = \"\"\n","    for alias in aliasses:\n","      s += alias + \"\\n\"\n","    with open(splits_path + f\"{name}_{keysname}_keys.txt\", \"w\") as f:\n","      f.write(s)\n","\n","    train_test_split(f\"{name}_{keysname}\", 0.2)"],"metadata":{"id":"X4PYeTSzcENX","executionInfo":{"status":"ok","timestamp":1680268483367,"user_tz":-120,"elapsed":892,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["load_biggraph([\"20K\", \"names\", \"places\", \"20K_1_to_1_synsets\", \"20K_2_to_3_synsets\", \"20K_4_to_infinity_synsets\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IN9tPjiyeDHg","executionInfo":{"status":"ok","timestamp":1680269623374,"user_tz":-120,"elapsed":840087,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"}},"outputId":"d265f5b9-370d-407f-cd0f-20fa972b4d9b"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Master Thesis/Reference spaces/biggraph_20K_reference_space.npy exists.\n","/content/drive/MyDrive/Master Thesis/Reference spaces/biggraph_names_reference_space.npy exists.\n","/content/drive/MyDrive/Master Thesis/Reference spaces/biggraph_places_reference_space.npy exists.\n","Loading model\n","2000\n","Making reference space\n","Size: 2000\n","Writing file\n","Loading model\n","2000\n","Making reference space\n","Size: 2000\n","Writing file\n","Loading model\n","2000\n","Making reference space\n","Size: 2000\n","Writing file\n"]}]},{"cell_type":"markdown","source":["#Load Graphvitre"],"metadata":{"id":"uVhZ0z9OQ4ut"}},{"cell_type":"code","source":["def load_graphvite(name, keysnames):\n","  with open(path + \"alias2entity.pickle\", \"rb\") as fin:\n","    alias2entity = pickle.load(fin)\n","\n","  embeddings_filename = f\"{name}_wikidata5m.pkl\"\n","  print(\"Loading model\")\n","  with open(path + embeddings_filename, \"rb\") as fin:\n","      model = pickle.load(fin)\n","\n","  for keysname in keysnames:\n","    if os.path.exists(reference_space_path+f\"{name}_{keysname}_reference_space.npy\"):\n","      print(reference_space_path+f\"{name}_{keysname}_reference_space.npy\" + \" exists.\")\n","      continue\n","    all_keys = load_keys(f\"{name}_{keysname}_keys.txt\", split=True)\n","\n","    entity_embeddings = model.solver.entity_embeddings\n","    entity2id = model.graph.entity2id\n","    aliasses = list(filter(lambda key: (key in alias2entity) and (alias2entity[key] in entity2id), all_keys))\n","\n","    print(f\"Making reference space, size: {len(aliasses)}\")\n","    reference_space = []\n","    for alias in aliasses:\n","      reference_space.append(entity_embeddings[entity2id[alias2entity[alias]]])\n","\n","    reference_space = np.array(reference_space)\n","    reference_space = np.append(reference_space, np.array(aliasses).reshape(len(aliasses),1), axis=1)\n","\n","    print(\"Saving files\")\n","    with open(reference_space_path+f\"{name}_{keysname}_reference_space.npy\", \"wb\") as f:\n","      np.save(f, reference_space)\n","\n","    s = \"\"\n","    for alias in aliasses:\n","      s += alias + \"\\n\"\n","    with open(splits_path + f\"{name}_{keysname}_keys.txt\", \"w\") as f:\n","      f.write(s)\n","\n","    train_test_split(f\"{name}_{keysname}\", 0.2)"],"metadata":{"id":"i7oGDQFVnLRu","executionInfo":{"status":"ok","timestamp":1680269737290,"user_tz":-120,"elapsed":738,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["load_graphvite(\"transe\", [\"20K\", \"names\", \"places\", \"20K_1_to_1_synsets\", \"20K_2_to_3_synsets\", \"20K_4_to_infinity_synsets\"])\n","load_graphvite(\"complex\", [\"20K\", \"names\", \"places\", \"20K_1_to_1_synsets\", \"20K_2_to_3_synsets\", \"20K_4_to_infinity_synsets\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kiqJ-9rHoE4h","executionInfo":{"status":"ok","timestamp":1680270196472,"user_tz":-120,"elapsed":456122,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"}},"outputId":"0552fc26-bfe0-4f8e-f1f7-55ac16d118f4"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model\n","/content/drive/MyDrive/Master Thesis/Reference spaces/transe_20K_reference_space.npy exists.\n","/content/drive/MyDrive/Master Thesis/Reference spaces/transe_names_reference_space.npy exists.\n","/content/drive/MyDrive/Master Thesis/Reference spaces/transe_places_reference_space.npy exists.\n","Making reference space, size: 2000\n","Saving files\n","Making reference space, size: 2000\n","Saving files\n","Making reference space, size: 2000\n","Saving files\n","Loading model\n","/content/drive/MyDrive/Master Thesis/Reference spaces/complex_20K_reference_space.npy exists.\n","/content/drive/MyDrive/Master Thesis/Reference spaces/complex_names_reference_space.npy exists.\n","/content/drive/MyDrive/Master Thesis/Reference spaces/complex_places_reference_space.npy exists.\n","Making reference space, size: 2000\n","Saving files\n","Making reference space, size: 2000\n","Saving files\n","Making reference space, size: 2000\n","Saving files\n"]}]},{"cell_type":"markdown","source":["#Load kgvec2go"],"metadata":{"id":"k3BSZyyX9T4Q"}},{"cell_type":"code","source":["def load_kgvec2go(keysnames):\n","  for keysname in keysnames:\n","    reference_space_filename = path+f\"kgvec2go_{keysname}_embeddings.npy\"\n","    keys_filename = splits_path + f\"kgvec2go_{keysname}_keys.txt\"\n","    if os.path.exists(reference_space_filename) and os.path.exists(keys_filename):\n","      continue\n","\n","    all_keys = load_keys(f\"{keysname}_keys.txt\", split=True)\n","    \n","    embeddings = []\n","    keys_used = []\n","    for i in tqdm(range(len(all_keys))):\n","      key = all_keys[i]\n","      URL = f\"http://www.kgvec2go.org/rest/get-vector/wiktionary/{key}\"\n","      r = requests.get(url = URL)\n","      data = r.json()\n","      try:\n","        embeddings.append(data[\"vector\"])\n","        keys_used.append(key)\n","      except:\n","        continue\n","\n","    reference_space = np.array(embeddings)\n","    reference_space = np.append(reference_space, np.array(keys_used).reshape(len(keys_used),1), axis=1)\n","\n","    with open(reference_space_filename, \"wb\") as f:\n","      np.save(f, reference_space)\n","\n","    s = \"\"\n","    for key in keys_used:\n","      s += key + \"\\n\"\n","    with open(keys_filename, \"w\") as f:\n","      f.write(s)\n","\n","    train_test_split(f\"kgvec2go_{keysname}\", 0.2)"],"metadata":{"id":"QzRS01OXpdqD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Generate synset keys based on 20K words\n","\n"],"metadata":{"id":"q3jE-EEV29MH"}},{"cell_type":"code","source":["import nltk\n","nltk.download('wordnet')\n","from nltk.corpus import wordnet as wn\n","import random"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DxqcCs2n3hBn","executionInfo":{"status":"ok","timestamp":1680254519071,"user_tz":-120,"elapsed":2,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"}},"outputId":"0d648f79-683a-49e1-8b2e-fb33b8a4edbf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}]},{"cell_type":"code","source":["def count_synsets(word):\n","  return len(wn.synsets(word))"],"metadata":{"id":"blD3HQJj3jUl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def count_synsets(word):\n","  return len(wn.synsets(word))\n","\n","def synset_split(reference_space):\n","  splits = [(1,1),(2,3),(4,sys.maxsize)]\n","  n_sample = 2000\n","  all_keys = load_keys(f\"{reference_space}_20K_keys.txt\", split=True)\n","\n","  synset_counter = {}\n","  for key in all_keys:\n","    n_synsets = count_synsets(key)\n","    if n_synsets not in synset_counter:\n","      synset_counter[n_synsets] = []\n","    synset_counter[n_synsets].append(key)\n","\n","  key_ranges = {}\n","  for split in splits:\n","    range_key = str(split[0])+\" to \"+(\"infinity\" if split[1] == sys.maxsize else str(split[1]))\n","    if range_key not in key_ranges:\n","      key_ranges[range_key] = []\n","    start = split[0]\n","    end = split[1]\n","    for key in synset_counter:\n","      if start <= key <= end:\n","        key_ranges[range_key].append(key)\n","  selected_keys = {}\n","  for key in key_ranges:\n","    seq = []\n","    for key2 in key_ranges[key]:\n","      seq += synset_counter[key2]\n","    assert(len(seq) == len(set(seq)))\n","    sample = random.sample(seq, n_sample)\n","    selected_keys[key] = sample\n","\n","  for key in selected_keys:\n","    keys_used = selected_keys[key]\n","    s = \"\"\n","    for key2 in keys_used:\n","      s += key2 + \"\\n\"\n","    key = key.replace(\" \", \"_\")\n","    with open(splits_path + f\"{reference_space}_20K_{key}_synsets_keys.txt\", \"w\") as f:\n","      f.write(s)\n","\n","    train_test_split(f\"{reference_space}_20K_{key}_synsets\", 0.2)"],"metadata":{"id":"-PoT7F6Y3UlU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["synset_split(\"biggraph\")\n","synset_split(\"transe\")\n","synset_split(\"complex\")"],"metadata":{"id":"ORtHNVufCtGL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gkYrcEP3RQkQ"},"execution_count":null,"outputs":[]}]}