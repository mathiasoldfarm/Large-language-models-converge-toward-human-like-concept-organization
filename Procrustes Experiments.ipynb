{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Yr4SZxVg32UC"},"outputs":[],"source":["%%capture\n","import pandas as pd\n","import numpy as np\n","%pip install ijson\n","import ijson\n","import gensim.downloader       \n","import random\n","from sklearn.decomposition import PCA\n","from scipy.linalg import orthogonal_procrustes\n","%pip install transformers\n","from transformers import AutoTokenizer, GPT2Model\n","import torch\n","%pip install sentence-transformers\n","from sentence_transformers import SentenceTransformer\n","import torch\n","import transformers\n","from transformers import BertTokenizer, BertModel\n","from sklearn.decomposition import PCA\n","from google.colab import drive\n","from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n","import matplotlib.pyplot as plt\n","import os\n","from sklearn.linear_model import LinearRegression\n","import math"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5612,"status":"ok","timestamp":1680360191186,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"},"user_tz":-120},"id":"uQroTSQi56nJ","outputId":"3df7537a-932e-4019-aa86-997bfaf76a82"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["drive.mount('/content/drive', force_remount=True)\n","path = \"/content/drive/MyDrive/Master Thesis/\"\n","log_path = path + \"Logs/\"\n","splits_path = path+\"splits/\"\n","reference_space_path = path + \"Reference spaces/\"\n","pca_embeddings_path = path + \"Generation of embeddings + experiments/Data/PCA embeddings/\"\n","cache_path = path+\"cache/\"\n","splitter = \"; \"\n","splitter2 = \": \"\n","\n","if torch.cuda.is_available():\n","  device = torch.device(\"cuda\")\n","else:\n","  device = torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4IqNGYcMF59L"},"outputs":[],"source":["def reset_log(log_filename):\n","  with open(log_filename, \"w\") as f:\n","    f.write(\"\")\n","\n","def write_line(obj, log_filename):\n","  s = \"\"\n","  counter = 0\n","  n = len(obj.keys())\n","  for key in obj:\n","    counter += 1\n","    s += f\"{key}{splitter2}{obj[key]}\"\n","    if counter != n:\n","      s += splitter\n","  s += \"\\n\"\n","  with open(log_filename, 'a') as f:\n","    f.write(s)\n","\n","def exists_in_log(models, ks, log_filename):\n","  with open(log_filename, \"r\") as f:\n","    content = f.readlines()\n","  models_target = set()\n","  ks_target = set()\n","  for line in content:\n","    line = line.replace(\"\\n\", \"\").split(splitter)\n","    assert(len(line) == 4)\n","    mapper = {}\n","    for elm in line:\n","      elm = elm.split(splitter2)\n","      assert(len(elm) == 2)\n","      key, value = elm[0], elm[1]\n","      mapper[key] = value\n","    assert(\"model\" in mapper and \"k\" in mapper)\n","    if mapper[\"model\"] in models:\n","      models_target.add(mapper[\"model\"])\n","      ks_target.add(int(mapper[\"k\"]))\n","  models = set(models)\n","  ks = set(ks)\n","  return models == models_target and ks == ks_target"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"36RYajg538sg"},"outputs":[],"source":["def load_keys(filename):\n","    with open(splits_path+filename, \"r\") as f:\n","        lines = f.readlines()\n","    keys = list(map(lambda line: line.replace(\"\\n\", \"\"), lines))\n","    return keys\n","\n","def load_lm_embeddings(model, keys, reference_space_key, key_type_key):\n","  embeddings = np.load(pca_embeddings_path + f\"{model}_{reference_space_key}_{key_type_key}_pca_embeddings.pkl\", allow_pickle=True).to_numpy()\n","  rows = []\n","  indexes = []\n","  added = set()\n","  for row in embeddings:\n","    key = row[0]\n","    if \"[CLS]\" in key:\n","      key = key.replace(\"[CLS] \", \"\")\n","    if \"[SEP]\" in key:\n","      key = key.replace(\" [SEP]\", \"\")\n","    if key in keys and key not in added:\n","      if isinstance(row[1], list):\n","        rows.append(np.array(row[1]).astype(float))\n","      else:\n","        rows.append(np.array(row[1].astype(float)))\n","      indexes.append(key)\n","      added.add(key)\n","  rows = np.array(rows)\n","\n","  indexes = np.array(indexes)\n","  df = pd.DataFrame(rows, index=indexes)\n","  return df\n","\n","def load_embeddings(model, filename, reference_space_key, key_type_key, clear_cache=False):\n","  cache_key = filename.replace(\".txt\", f\"{model}_{reference_space_key}_{key_type_key}_embeddings_procrustes_cache.pkl\")\n","  if not clear_cache:\n","    cache = try_load_df_cache(cache_key)\n","    if cache is not None:\n","      return cache\n","\n","  keys = load_keys(filename)\n","  retval = None\n","  if model in [\"bert-base\", \"bert-mini\", \"bert-small\", \"bert-medium\", \"bert-large\", \"XLNet-base\", \"XLNet-large\", \"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\", \"ada-002\", \"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\", \"opt-125m\", \"opt-350m\", \"opt-1.3b\", \"opt-2.7b\", \"opt-6.7b\"]:\n","    retval = load_lm_embeddings(model, keys, reference_space_key, key_type_key)\n","  if retval is None:\n","    raise Exception(\"Unknown model\")\n","  retval = retval.sort_index()\n","  write_to_df_cache(retval, cache_key)\n","  return retval\n","\n","def try_load_df_cache(filename):\n","  assert(\".pkl\" in filename)\n","  if os.path.exists(cache_path + filename):\n","    return pd.read_pickle(cache_path + filename)\n","  return None\n","\n","def write_to_df_cache(df, filename):\n","  df.to_pickle(cache_path + filename) \n","\n","def load_reference_space(filename, reference_space_key, key_type_key, clear_cache=False):\n","  cache_key = filename.replace(\".txt\", f\"{reference_space_key}_{key_type_key}_reference_space_procrustes_cache.pkl\")\n","  if not clear_cache:\n","    cache = try_load_df_cache(cache_key)\n","    if cache is not None:\n","      return cache\n","\n","  keys = load_keys(filename)\n","  reference_space_filename = f'{reference_space_key}_{key_type_key}_reference_space.npy'\n","  X = np.load(reference_space_path + reference_space_filename, mmap_mode='r')\n","  rows = []\n","  indexes = []\n","  added = set()\n","  n_cols = X.shape[1]\n","  for row in X:\n","    if row[n_cols-1] in keys and row[n_cols-1] not in added:\n","      rows.append(np.array(row[:n_cols-1].astype(float)))\n","      indexes.append(row[n_cols-1])\n","      added.add(row[n_cols-1])\n","\n","  rows = np.array(rows)\n","\n","  indexes = np.array(indexes)\n","  df = pd.DataFrame(rows, index=indexes)\n","  df = df.sort_index()\n","  write_to_df_cache(df, cache_key)\n","  return df\n","\n","def cosine(a,b):\n","  cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n","  return cos_sim\n","\n","def precision_at_k_analysis(source, target, k):\n","  n = source.shape[0]\n","  corrects_cosine = []\n","  corrects_euclidian = []\n","\n","  distances_euclidian = euclidean_distances(source, target)\n","  distances_euclidian = pd.DataFrame(distances_euclidian, index=source.index, columns=source.index)\n","  for key in distances_euclidian.index:\n","    distances = distances_euclidian.loc[key].sort_values(ascending=True)[:k]\n","    corrects_euclidian.append(1 if key in distances.index else 0)\n","\n","  distances_cosine = cosine_similarity(source.values, target.values)\n","  distances_cosine = pd.DataFrame(distances_cosine, index=source.index, columns=source.index)\n","  for key in distances_cosine.index:\n","    distances = distances_cosine.loc[key].sort_values(ascending=False)[:k]\n","    corrects_cosine.append(1 if key in distances.index else 0)\n","\n","  return sum(corrects_cosine) / len(corrects_cosine), sum(corrects_euclidian) / len(corrects_euclidian)\n","\n","def translate(df):\n","  indexes = df.index\n","  arr = df.to_numpy()\n","  arr -= np.mean(arr, 0)\n","  arr /= np.linalg.norm(arr)\n","  df = pd.DataFrame(arr, index = indexes)\n","  return df\n","\n","def transform(df, R, sca):\n","  return df.dot(R) * sca\n","\n","def cosine_test(a,b):\n","  return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n"]},{"cell_type":"markdown","metadata":{"id":"578DlfrR4SIn"},"source":["#Experiments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8A3pdgR8E4lf"},"outputs":[],"source":["bert_models = [\"bert-mini\", \"bert-small\", \"bert-medium\", \"bert-base\", \"bert-large\"]\n","t5_models = [\"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\"]\n","opt_models = [\"opt-125m\", \"opt-350m\", \"opt-1.3b\", \"opt-2.7b\", \"opt-6.7b\"]\n","gpt_models = ['gpt2','gpt2-medium','gpt2-large', \"gpt2-xl\", \"ada-002\"]\n","XLNet_models = [\"XLNet-base\", \"XLNet-large\"]\n","all_models = [bert_models, t5_models, opt_models, gpt_models, XLNet_models]\n","ks = [1,10,20,50]"]},{"cell_type":"code","source":["def run_experiment(models, train_filename, test_filename, reference_space_key, key_type_key, log_filename):\n","  if not exists_in_log(models, ks, log_filename):\n","    print(f\"Experiment for: models: {models}\")\n","    print(\"Loading referfence space\")\n","    clear_cache=True\n","    y_train, y_test = load_reference_space(train_filename, reference_space_key, key_type_key, clear_cache=clear_cache), load_reference_space(test_filename, reference_space_key, key_type_key, clear_cache=clear_cache)\n","\n","    for model in models:\n","      lm_filepath = pca_embeddings_path + f\"{model}_{reference_space_key}_{key_type_key}_pca_embeddings.pkl\"\n","      if not os.path.exists(lm_filepath):\n","        print(f\"Skipping {model}\")\n","        continue\n","\n","      print(f\"Running model: {model}\")\n","      print(\"Loading embeddings\")\n","      X_train, X_test = load_embeddings(model, train_filename, reference_space_key, key_type_key, clear_cache=clear_cache), load_embeddings(model, test_filename, reference_space_key, key_type_key, clear_cache=clear_cache)\n","      print(f\"Train size: {len(X_train.index.to_list())}, test_size: {len(X_test.index.to_list())}\")\n","      assert(len(set(X_train.index.to_list())) == len(X_train.index.to_list()))\n","      assert(len(set(X_test.index.to_list())) == len(X_test.index.to_list()))\n","      \n","      print(\"Doing procrustes\")\n","      translate(y_train)            \n","      translate(X_train)\n","      R, sca = orthogonal_procrustes(X_train, y_train)\n","\n","      translate(y_test)\n","      translate(X_test)\n","      y_test_pred = transform(X_test, R, sca)\n","\n","      print(\"Evaluating\")\n","      for k in ks:\n","        test_acc_cosine, test_acc_euclidian = precision_at_k_analysis(y_test_pred, y_test, k)\n","        write_line({\"model\": model, \"k\": k, \"test P@K cosine\": test_acc_cosine, \"test P@K euclidian\": test_acc_euclidian}, log_filename)\n","  else:\n","    print(f\"Already exists in log: models: {models}\")"],"metadata":{"id":"ShozqTZ2ieD1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NYiFl0AgFAAH"},"outputs":[],"source":["def run_all(reference_space_key, key_type_key):\n","  log_filename = log_path + f\"log_procrustes_{reference_space_key}_{key_type_key}.txt\"\n","  test_filename = f\"test_{reference_space_key}_{key_type_key}.txt\"\n","  train_filename = f\"train_{reference_space_key}_{key_type_key}.txt\"\n","\n","  if os.path.exists(log_filename):\n","    print(f\"Experiment data exists: {reference_space_key}, {key_type_key}\")\n","  else:\n","    reset_log(log_filename)\n","    for models in all_models:\n","      run_experiment(models, train_filename, test_filename, reference_space_key, key_type_key, log_filename)"]},{"cell_type":"code","source":["reference_spaces = [\"biggraph\", \"transe\", \"complex\"]\n","key_type_keys = [\"20K\", \"places\", \"names\", \"20K_1_to_1_synsets\", \"20K_2_to_3_synsets\", \"20K_4_to_infinity_synsets\"]\n","\n","for reference_space in reference_spaces:\n","  for key_type_key in key_type_keys:\n","    run_all(reference_space, key_type_key)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G0QJXKcjpDdu","executionInfo":{"status":"ok","timestamp":1680363661806,"user_tz":-120,"elapsed":6,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"}},"outputId":"2e4924ac-84e1-430c-dfeb-9c309c601c92"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Experiment data exists: biggraph, 20K\n","Experiment data exists: biggraph, places\n","Experiment data exists: biggraph, names\n","Experiment data exists: biggraph, 20K_1_to_1_synsets\n","Experiment data exists: biggraph, 20K_2_to_3_synsets\n","Experiment data exists: biggraph, 20K_4_to_infinity_synsets\n","Experiment data exists: transe, 20K\n","Experiment data exists: transe, places\n","Experiment data exists: transe, names\n","Experiment data exists: transe, 20K_1_to_1_synsets\n","Experiment data exists: transe, 20K_2_to_3_synsets\n","Experiment data exists: transe, 20K_4_to_infinity_synsets\n","Experiment data exists: complex, 20K\n","Experiment data exists: complex, places\n","Experiment data exists: complex, names\n","Experiment data exists: complex, 20K_1_to_1_synsets\n","Experiment data exists: complex, 20K_2_to_3_synsets\n","Experiment data exists: complex, 20K_4_to_infinity_synsets\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"57qgQQc1eqZT"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}