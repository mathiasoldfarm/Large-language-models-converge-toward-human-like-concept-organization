{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9506,"status":"ok","timestamp":1682950782518,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"},"user_tz":-120},"id":"I_a9ogLve6Kt","outputId":"84efde65-c99b-4fd4-f9b8-cdb5bb6cff23"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: easydict in /usr/local/lib/python3.10/dist-packages (1.10)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ijson\n","  Downloading ijson-3.2.0.post0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (113 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.3/113.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: ijson\n","Successfully installed ijson-3.2.0.post0\n"]}],"source":["!pip install easydict\n","!pip install ijson"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":516,"status":"ok","timestamp":1682950850026,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"},"user_tz":-120},"id":"ZIEAZPnrRAk3"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from google.colab import drive\n","from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression\n","import gensim.downloader\n","import os\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n","from sklearn.preprocessing import normalize\n","from sklearn import ensemble\n","import matplotlib.pyplot as plt\n","import random\n","import torch\n","import torch.nn as nn\n","from tqdm import tqdm\n","import math\n","import sys\n","import nltk\n","import pickle\n","import easydict\n","import requests\n","import ijson\n","import re"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5471,"status":"ok","timestamp":1682950856073,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"},"user_tz":-120},"id":"Rb6Qc2pGQ3jf","outputId":"6ad3ccaf-9585-4269-ba8a-50658f828873"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["drive.mount('/content/drive', force_remount=True)\n","path = \"/content/drive/MyDrive/Master Thesis/\"\n","reference_space_path = \"/content/drive/MyDrive/Master Thesis/Reference spaces/\"\n","splits_path = path+\"splits/\""]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1682950856074,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"},"user_tz":-120},"id":"7ujFMuVJQhbb"},"outputs":[],"source":["def train_test_split(name, test_size):\n","  train_size = 1 - test_size\n","  with open(splits_path+f\"{name}_keys.txt\", 'r', encoding=\"utf-8\") as f:\n","        lines = f.readlines()\n","  test_n = int(len(lines) * test_size)\n","  indicies = set(random.sample(range(len(lines)), test_n))\n","  train = []\n","  test = []\n","  for i, line in enumerate(lines):\n","      if i in indicies: test.append(line)\n","      else: train.append(line)\n","  with open(splits_path+f\"train_{name}.txt\", 'w') as f:\n","    f.writelines(train)\n","\n","  with open(splits_path+f\"test_{name}.txt\", 'w') as f:\n","    f.writelines(test)\n","\n","def load_keys(filename, split=False, nltk_filter=None):\n","    with open(splits_path+filename if split else path+filename, \"r\") as f:\n","        lines = f.readlines()\n","    #random.shuffle(lines)\n","    keys = list(map(lambda line: line.replace(\"\\n\", \"\").lower(), lines))\n","    if nltk_filter is not None:\n","      nltk.download('averaged_perceptron_tagger')\n","      filtered_keys = []\n","      for key in keys:\n","        tag = nltk.pos_tag([key])\n","        if tag[0][1] in nltk_filter:\n","          filtered_keys.append(tag[0][0])\n","      return filtered_keys\n","    return set(keys)"]},{"cell_type":"markdown","metadata":{"id":"mwWKFPkYRRbX"},"source":["#Biggraph"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1682950856074,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"},"user_tz":-120},"id":"X4PYeTSzcENX"},"outputs":[],"source":["def load_biggraph(keysnames):\n","  name = \"biggraph\"  \n","  for keysname in keysnames:\n","    if os.path.exists(reference_space_path+f\"{name}_{keysname}_reference_space.npy\"):\n","      print(reference_space_path+f\"{name}_{keysname}_reference_space.npy\" + \" exists.\")\n","      continue\n","    print(\"Loading model\")\n","    f = open(path+\"wikidata_translation_v1_names.json\", 'r', encoding=\"utf-8\")\n","    objs = ijson.items(f, 'item')\n","\n","    biggraph = np.load(path+'wikidata_translation_v1_vectors.npy', mmap_mode='r')\n","\n","    all_keys = load_keys(f\"{keysname}_keys.txt\", split=True)\n","    print(len(all_keys))\n","    \n","    aliasses = []\n","    idx = []\n","    print(\"Making reference space\")\n","    for i, o in enumerate(objs):\n","      word = o.lower().replace(\"\\\"\", \"\")\n","      word = re.sub(\"\\@(.*)\", \"\", word)\n","      if word in all_keys and word not in aliasses:\n","        aliasses.append(word)\n","        idx.append(i)\n","    reference_space = biggraph[idx]\n","    print(f\"Size: {len(aliasses)}\")\n","\n","    reference_space = np.append(reference_space, np.array(aliasses).reshape(len(aliasses),1), axis=1)\n","    print(\"Writing file\")\n","    with open(reference_space_path+f\"{name}_{keysname}_reference_space.npy\", \"wb\") as f:\n","        np.save(f, reference_space)\n","\n","    s = \"\"\n","    for alias in aliasses:\n","      s += alias + \"\\n\"\n","    with open(splits_path + f\"{name}_{keysname}_keys.txt\", \"w\") as f:\n","      f.write(s)\n","\n","    train_test_split(f\"{name}_{keysname}\", 0.2)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30988,"status":"ok","timestamp":1682952338679,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"},"user_tz":-120},"id":"IN9tPjiyeDHg"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Master Thesis/Reference spaces/biggraph_20K_reference_space.npy exists.\n","/content/drive/MyDrive/Master Thesis/Reference spaces/biggraph_names_reference_space.npy exists.\n","/content/drive/MyDrive/Master Thesis/Reference spaces/biggraph_places_reference_space.npy exists.\n","Loading model\n","2000\n","Making reference space\n","Size: 2000\n","Writing file\n","Loading model\n","2000\n","Making reference space\n","Size: 2000\n","Writing file\n","Loading model\n","2000\n","Making reference space\n","Size: 2000\n","Writing file\n"]}],"source":["load_biggraph([\"20K\", \"names\", \"places\", \"20K_1_to_1_synsets\", \"20K_2_to_3_synsets\", \"20K_4_to_infinity_synsets\"])"]},{"cell_type":"markdown","metadata":{"id":"uVhZ0z9OQ4ut"},"source":["#Load Graphvitre"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1682952338680,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"},"user_tz":-120},"id":"i7oGDQFVnLRu"},"outputs":[],"source":["def load_graphvite(name, keysnames):\n","  with open(path + \"alias2entity.pickle\", \"rb\") as fin:\n","    alias2entity = pickle.load(fin)\n","  print(len(alias2entity))\n","\n","  embeddings_filename = f\"{name}_wikidata5m.pkl\"\n","  print(\"Loading model\")\n","  with open(path + embeddings_filename, \"rb\") as fin:\n","      model = pickle.load(fin)\n","\n","  for keysname in keysnames:\n","    if os.path.exists(reference_space_path+f\"{name}_{keysname}_reference_space.npy\"):\n","      print(reference_space_path+f\"{name}_{keysname}_reference_space.npy\" + \" exists.\")\n","      continue\n","    all_keys = load_keys(f\"{keysname}_keys.txt\", split=True)\n","    print(len(all_keys))\n","\n","    entity_embeddings = model.solver.entity_embeddings\n","    entity2id = model.graph.entity2id\n","    aliasses = list(filter(lambda key: (key in alias2entity) and (alias2entity[key] in entity2id), all_keys))\n","\n","    print(f\"Making reference space, size: {len(aliasses)}\")\n","    reference_space = []\n","    for alias in aliasses:\n","      reference_space.append(entity_embeddings[entity2id[alias2entity[alias]]])\n","\n","    reference_space = np.array(reference_space)\n","    reference_space = np.append(reference_space, np.array(aliasses).reshape(len(aliasses),1), axis=1)\n","\n","    print(\"Saving files\")\n","    with open(reference_space_path+f\"{name}_{keysname}_reference_space.npy\", \"wb\") as f:\n","      np.save(f, reference_space)\n","\n","    s = \"\"\n","    for alias in aliasses:\n","      s += alias + \"\\n\"\n","    with open(splits_path + f\"{name}_{keysname}_keys.txt\", \"w\") as f:\n","      f.write(s)\n","\n","    train_test_split(f\"{name}_{keysname}\", 0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kiqJ-9rHoE4h"},"outputs":[],"source":["load_graphvite(\"transe\", [\"20K\", \"names\", \"places\", \"20K_1_to_1_synsets\", \"20K_2_to_3_synsets\", \"20K_4_to_infinity_synsets\"])\n","load_graphvite(\"complex\", [\"20K\", \"names\", \"places\", \"20K_1_to_1_synsets\", \"20K_2_to_3_synsets\", \"20K_4_to_infinity_synsets\"])"]},{"cell_type":"markdown","metadata":{"id":"k3BSZyyX9T4Q"},"source":["#Load kgvec2go"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1682950840783,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"},"user_tz":-120},"id":"QzRS01OXpdqD"},"outputs":[],"source":["def load_kgvec2go(keysnames):\n","  for keysname in keysnames:\n","    reference_space_filename = path+f\"kgvec2go_{keysname}_embeddings.npy\"\n","    keys_filename = splits_path + f\"kgvec2go_{keysname}_keys.txt\"\n","    if os.path.exists(reference_space_filename) and os.path.exists(keys_filename):\n","      continue\n","\n","    all_keys = load_keys(f\"{keysname}_keys.txt\", split=True)\n","    \n","    embeddings = []\n","    keys_used = []\n","    for i in tqdm(range(len(all_keys))):\n","      key = all_keys[i]\n","      URL = f\"http://www.kgvec2go.org/rest/get-vector/wiktionary/{key}\"\n","      r = requests.get(url = URL)\n","      data = r.json()\n","      try:\n","        embeddings.append(data[\"vector\"])\n","        keys_used.append(key)\n","      except:\n","        continue\n","\n","    reference_space = np.array(embeddings)\n","    reference_space = np.append(reference_space, np.array(keys_used).reshape(len(keys_used),1), axis=1)\n","\n","    with open(reference_space_filename, \"wb\") as f:\n","      np.save(f, reference_space)\n","\n","    s = \"\"\n","    for key in keys_used:\n","      s += key + \"\\n\"\n","    with open(keys_filename, \"w\") as f:\n","      f.write(s)\n","\n","    train_test_split(f\"kgvec2go_{keysname}\", 0.2)"]},{"cell_type":"markdown","metadata":{"id":"q3jE-EEV29MH"},"source":["#Generate synset keys based on 20K words\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1682950840783,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"},"user_tz":-120},"id":"DxqcCs2n3hBn"},"outputs":[],"source":["import nltk\n","nltk.download('wordnet')\n","from nltk.corpus import wordnet as wn\n","import random"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1682950840784,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"},"user_tz":-120},"id":"blD3HQJj3jUl"},"outputs":[],"source":["def count_synsets(word):\n","  return len(wn.synsets(word))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1682950840784,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"},"user_tz":-120},"id":"-PoT7F6Y3UlU"},"outputs":[],"source":["def count_synsets(word):\n","  return len(wn.synsets(word))\n","\n","def synset_split(reference_space):\n","  splits = [(1,1),(2,3),(4,sys.maxsize)]\n","  n_sample = 2000\n","  all_keys = load_keys(f\"{reference_space}_20K_keys.txt\", split=True)\n","\n","  synset_counter = {}\n","  for key in all_keys:\n","    n_synsets = count_synsets(key)\n","    if n_synsets not in synset_counter:\n","      synset_counter[n_synsets] = []\n","    synset_counter[n_synsets].append(key)\n","\n","  key_ranges = {}\n","  for split in splits:\n","    range_key = str(split[0])+\" to \"+(\"infinity\" if split[1] == sys.maxsize else str(split[1]))\n","    if range_key not in key_ranges:\n","      key_ranges[range_key] = []\n","    start = split[0]\n","    end = split[1]\n","    for key in synset_counter:\n","      if start \u003c= key \u003c= end:\n","        key_ranges[range_key].append(key)\n","  selected_keys = {}\n","  for key in key_ranges:\n","    seq = []\n","    for key2 in key_ranges[key]:\n","      seq += synset_counter[key2]\n","    assert(len(seq) == len(set(seq)))\n","    sample = random.sample(seq, n_sample)\n","    selected_keys[key] = sample\n","\n","  for key in selected_keys:\n","    keys_used = selected_keys[key]\n","    s = \"\"\n","    for key2 in keys_used:\n","      s += key2 + \"\\n\"\n","    key = key.replace(\" \", \"_\")\n","    with open(splits_path + f\"{reference_space}_20K_{key}_synsets_keys.txt\", \"w\") as f:\n","      f.write(s)\n","\n","    train_test_split(f\"{reference_space}_20K_{key}_synsets\", 0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"aborted","timestamp":1682950840784,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"},"user_tz":-120},"id":"ORtHNVufCtGL"},"outputs":[],"source":["synset_split(\"biggraph\")\n","synset_split(\"transe\")\n","synset_split(\"complex\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gkYrcEP3RQkQ"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyP0aqlnpMXZ7VIUdgqsFfzs","machine_shape":"hm","name":"","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}