{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4793,"status":"ok","timestamp":1680363708309,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"},"user_tz":-120},"id":"nbdtRYhd_Hl6"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from google.colab import drive\n","from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression\n","import gensim.downloader\n","import os\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n","from sklearn.preprocessing import normalize\n","from sklearn import ensemble\n","import matplotlib.pyplot as plt\n","import random\n","import torch\n","import torch.nn as nn\n","from tqdm import tqdm\n","import math\n","import nltk\n","import re"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20444,"status":"ok","timestamp":1680363728747,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"},"user_tz":-120},"id":"_HLvcYBXA6Vd","outputId":"1cab22a7-8bc5-46e7-fe4c-5579dcac9049"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["drive.mount('/content/drive', force_remount=True)\n","path = \"/content/drive/MyDrive/Master Thesis/\"\n","log_path = path + \"Logs/\"\n","splits_path = path+\"splits/\"\n","reference_space_path = path + \"Reference spaces/\"\n","regular_embeddings_path = path + \"Generation of embeddings + experiments/Data/Regular embeddings/\"\n","cache_path = path+\"cache/\"\n","splitter = \"; \"\n","splitter2 = \": \"\n","\n","if torch.cuda.is_available():\n","  device = torch.device(\"cuda\")\n","else:\n","  device = torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1680363728748,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"},"user_tz":-120},"id":"SwznN9HBN0xU"},"outputs":[],"source":["def reset_log(log_filename):\n","  with open(log_filename, \"w\") as f:\n","    f.write(\"\")\n","\n","def write_line(obj, log_filename):\n","  s = \"\"\n","  counter = 0\n","  n = len(obj.keys())\n","  for key in obj:\n","    counter += 1\n","    s += f\"{key}{splitter2}{obj[key]}\"\n","    if counter != n:\n","      s += splitter\n","  s += \"\\n\"\n","  with open(log_filename, 'a') as f:\n","    f.write(s)\n","\n","def exists_in_log(models, ks, log_filename):\n","  with open(log_filename, \"r\") as f:\n","    content = f.readlines()\n","  models_target = set()\n","  ks_target = set()\n","  for line in content:\n","    line = line.replace(\"\\n\", \"\").split(splitter)\n","    assert(len(line) == 4)\n","    mapper = {}\n","    for elm in line:\n","      elm = elm.split(splitter2)\n","      assert(len(elm) == 2)\n","      key, value = elm[0], elm[1]\n","      mapper[key] = value\n","    assert(\"model\" in mapper and \"k\" in mapper)\n","    if mapper[\"model\"] in models:\n","      models_target.add(mapper[\"model\"])\n","      ks_target.add(int(mapper[\"k\"]))\n","  models = set(models)\n","  ks = set(ks)\n","  return models == models_target and ks == ks_target"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1680363728749,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"},"user_tz":-120},"id":"myZUz-NHAyQH"},"outputs":[],"source":["def load_keys(filename):\n","    with open(splits_path+filename, \"r\") as f:\n","        lines = f.readlines()\n","    keys = list(map(lambda line: line.replace(\"\\n\", \"\"), lines))\n","    return keys\n","\n","def load_lm_embeddings(model, keys, reference_space_key, key_type_key):\n","  embeddings = np.load(regular_embeddings_path + f\"{model}_{reference_space_key}_{key_type_key}_embeddings.pkl\", allow_pickle=True).to_numpy()\n","  rows = []\n","  indexes = []\n","  added = set()\n","  for row in embeddings:\n","    key = row[0]\n","    if \"[CLS]\" in key:\n","      key = key.replace(\"[CLS] \", \"\")\n","    if \"[SEP]\" in key:\n","      key = key.replace(\" [SEP]\", \"\")\n","    if key in keys and key not in added:\n","      if isinstance(row[1], list):\n","        rows.append(np.array(row[1]).astype(float))\n","      else:\n","        rows.append(np.array(row[1].astype(float)))\n","      indexes.append(key)\n","      added.add(key)\n","  rows = np.array(rows)\n","\n","  indexes = np.array(indexes)\n","  df = pd.DataFrame(rows, index=indexes)\n","  return df\n","\n","def load_embeddings(model, filename, reference_space_key, key_type_key, clear_cache=False):\n","  cache_key = filename.replace(\".txt\", f\"{model}_{reference_space_key}_{key_type_key}_embeddings_ridge_cache.pkl\")\n","  if not clear_cache:\n","    cache = try_load_df_cache(cache_key)\n","    if cache is not None:\n","      return cache\n","\n","  keys = load_keys(filename)\n","  retval = None\n","  if model in [\"bert-base\", \"bert-mini\", \"bert-small\", \"bert-medium\", \"bert-large\", \"XLNet-base\", \"XLNet-large\", \"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\", \"ada-002\", \"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\", \"opt-125m\", \"opt-350m\", \"opt-1.3b\", \"opt-2.7b\", \"opt-6.7b\"]:\n","    retval = load_lm_embeddings(model, keys, reference_space_key, key_type_key)\n","  if retval is None:\n","    raise Exception(\"Unknown model\")\n","  retval = retval.sort_index()\n","  write_to_df_cache(retval, cache_key)\n","  return retval\n","\n","def try_load_df_cache(filename):\n","  assert(\".pkl\" in filename)\n","  if os.path.exists(cache_path + filename):\n","    return pd.read_pickle(cache_path + filename)\n","  return None\n","\n","def write_to_df_cache(df, filename):\n","  df.to_pickle(cache_path + filename) \n","\n","def load_reference_space(filename, reference_space_key, key_type_key, clear_cache=False):\n","  cache_key = filename.replace(\".txt\", f\"{reference_space_key}_{key_type_key}_reference_space_ridge_cache.pkl\")\n","  if not clear_cache:\n","    cache = try_load_df_cache(cache_key)\n","    if cache is not None:\n","      return cache\n","\n","  keys = load_keys(filename)\n","  reference_space_filename = f'{reference_space_key}_{key_type_key}_reference_space.npy'\n","  X = np.load(reference_space_path + reference_space_filename, mmap_mode='r')\n","  rows = []\n","  indexes = []\n","  added = set()\n","  n_cols = X.shape[1]\n","  for row in X:\n","    if row[n_cols-1] in keys and row[n_cols-1] not in added:\n","      rows.append(np.array(row[:n_cols-1].astype(float)))\n","      indexes.append(row[n_cols-1])\n","      added.add(row[n_cols-1])\n","\n","  rows = np.array(rows)\n","\n","  indexes = np.array(indexes)\n","  df = pd.DataFrame(rows, index=indexes)\n","  df = df.sort_index()\n","  write_to_df_cache(df, cache_key)\n","  return df\n","\n","def cosine(a,b):\n","  cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n","  return cos_sim\n","\n","def precision_at_k_analysis(source, target, k):\n","  n = source.shape[0]\n","  corrects_cosine = []\n","  corrects_euclidian = []\n","\n","  distances_euclidian = euclidean_distances(source, target)\n","  distances_euclidian = pd.DataFrame(distances_euclidian, index=source.index, columns=source.index)\n","  for key in distances_euclidian.index:\n","    distances = distances_euclidian.loc[key].sort_values(ascending=True)[:k]\n","    corrects_euclidian.append(1 if key in distances.index else 0)\n","\n","  distances_cosine = cosine_similarity(source.values, target.values)\n","  distances_cosine = pd.DataFrame(distances_cosine, index=source.index, columns=source.index)\n","  for key in distances_cosine.index:\n","    distances = distances_cosine.loc[key].sort_values(ascending=False)[:k]\n","    corrects_cosine.append(1 if key in distances.index else 0)\n","\n","  return sum(corrects_cosine) / len(corrects_cosine), sum(corrects_euclidian) / len(corrects_euclidian)\n","\n","def train(X, y):\n","  #reg = LinearRegression()\n","  reg = Ridge(alpha=10)\n","  reg.fit(X, y)\n","  return lambda _X: reg.predict(_X)\n","  "]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1680363728749,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"},"user_tz":-120},"id":"t2ZK3gevODST"},"outputs":[],"source":["bert_models = [\"bert-mini\", \"bert-small\", \"bert-medium\", \"bert-base\", \"bert-large\"]\n","t5_models = [\"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\"]\n","opt_models = [\"opt-125m\", \"opt-350m\", \"opt-1.3b\", \"opt-2.7b\", \"opt-6.7b\"]\n","gpt_models = ['gpt2','gpt2-medium','gpt2-large', \"gpt2-xl\", \"ada-002\"]\n","XLNet_models = [\"XLNet-base\", \"XLNet-large\"]\n","word2vec_models = ['glove-wiki-gigaword-200', 'glove-wiki-gigaword-300']\n","titles = [\"BERT\", \"T5\", \"OPT\", \"GPT\", \"XLNet\"]\n","all_models = [bert_models, t5_models, opt_models, gpt_models, XLNet_models]\n","ks = [1,10,20,50]"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1680363728750,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"},"user_tz":-120},"id":"ejlLBFmyN8Vn"},"outputs":[],"source":["def make_embeddings(X, mappings, d):\n","  columns = []\n","  for i in range(d):\n","    columns.append(pd.Series(mappings[i](X)))\n","  df=pd.concat(columns,axis=1)\n","  df = pd.DataFrame(df.values, index=X.index)\n","  return df\n","\n","def run_experiment(models, train_filename, test_filename, reference_space_key, key_type_key, d, log_filename):\n","  if not exists_in_log(models, ks, log_filename):\n","    print(f\"Experiment for: models: {models}\")\n","    print(\"Loading referfence space\")\n","    clear_cache=True\n","    y_train, y_test = load_reference_space(train_filename, reference_space_key, key_type_key, clear_cache=clear_cache), load_reference_space(test_filename, reference_space_key, key_type_key, clear_cache=clear_cache)\n","\n","    for model in models:\n","      print(f\"Running model: {model}\")\n","      print(\"Loading embeddings\")\n","      X_train, X_test = load_embeddings(model, train_filename, reference_space_key, key_type_key, clear_cache=clear_cache), load_embeddings(model, test_filename, reference_space_key, key_type_key, clear_cache=clear_cache)\n","      print(f\"Train size: {len(X_train.index.to_list())}, test_size: {len(X_test.index.to_list())}\")\n","      assert(len(set(X_train.index.to_list())) == len(X_train.index.to_list()))\n","      assert(len(set(X_test.index.to_list())) == len(X_test.index.to_list()))\n","      print(\"Training mappings\")\n","      mappings = []\n","      for j in tqdm(range(d)):\n","          mapping = train(X_train, y_train.iloc[:, j])\n","          mappings.append(mapping)\n","      print(\"Predicting\")\n","      y_test_pred = make_embeddings(X_test, mappings, d)\n","\n","      print(\"Evaluating\")\n","      for k in ks:\n","        test_acc_cosine, test_acc_euclidian = precision_at_k_analysis(y_test_pred, y_test, k)\n","        write_line({\"model\": model, \"k\": k, \"test P@K cosine\": test_acc_cosine, \"test P@K euclidian\": test_acc_euclidian}, log_filename)\n","  else:\n","    print(f\"Already exists in log: models: {models}\")"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1680363728750,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"},"user_tz":-120},"id":"YUqRI0IFsx_T"},"outputs":[],"source":["def run_all(reference_space_key, key_type_key, d):\n","  log_filename = log_path + f\"log_ridge_{reference_space_key}_{key_type_key}.txt\"\n","  test_filename = f\"test_{reference_space_key}_{key_type_key}.txt\"\n","  train_filename = f\"train_{reference_space_key}_{key_type_key}.txt\"\n","\n","  if os.path.exists(log_filename):\n","    print(f\"Experiment data exists: {reference_space_key}, {key_type_key}\")\n","  else:\n","    reset_log(log_filename)\n","    for models in all_models:\n","      run_experiment(models, train_filename, test_filename, reference_space_key, key_type_key, d, log_filename)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3iI9h13ix17v","executionInfo":{"status":"ok","timestamp":1680363730596,"user_tz":-120,"elapsed":1857,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"}},"outputId":"59867f94-9a48-4214-dab3-1754e705b004"},"outputs":[{"output_type":"stream","name":"stdout","text":["Experiment data exists: biggraph, 20K\n","Experiment data exists: biggraph, places\n","Experiment data exists: biggraph, names\n","Experiment data exists: biggraph, 20K_1_to_1_synsets\n","Experiment data exists: biggraph, 20K_2_to_3_synsets\n","Experiment data exists: biggraph, 20K_4_to_infinity_synsets\n","Experiment data exists: transe, 20K\n","Experiment data exists: transe, places\n","Experiment data exists: transe, names\n","Experiment data exists: transe, 20K_1_to_1_synsets\n","Experiment data exists: transe, 20K_2_to_3_synsets\n","Experiment data exists: transe, 20K_4_to_infinity_synsets\n","Experiment data exists: complex, 20K\n","Experiment data exists: complex, places\n","Experiment data exists: complex, names\n","Experiment data exists: complex, 20K_1_to_1_synsets\n","Experiment data exists: complex, 20K_2_to_3_synsets\n","Experiment data exists: complex, 20K_4_to_infinity_synsets\n"]}],"source":["reference_spaces = [(\"biggraph\", 200), (\"transe\", 512), (\"complex\", 512)]\n","key_type_keys = [\"20K\", \"places\", \"names\", \"20K_1_to_1_synsets\", \"20K_2_to_3_synsets\", \"20K_4_to_infinity_synsets\"]\n","\n","for reference_space, d in reference_spaces:\n","  for key_type_key in key_type_keys:\n","    run_all(reference_space, key_type_key, d)"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyPhssp2WXFxItoC7LIzpYUW"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}