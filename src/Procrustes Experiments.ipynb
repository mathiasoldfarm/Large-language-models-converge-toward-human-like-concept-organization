{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":58337,"status":"ok","timestamp":1683644630593,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"},"user_tz":-120},"id":"Yr4SZxVg32UC"},"outputs":[],"source":["%%capture\n","import pandas as pd\n","import numpy as np\n","%pip install ijson\n","import ijson\n","import gensim.downloader       \n","import random\n","from sklearn.decomposition import PCA\n","from scipy.linalg import orthogonal_procrustes\n","%pip install transformers\n","from transformers import AutoTokenizer, GPT2Model\n","import torch\n","%pip install sentence-transformers\n","from sentence_transformers import SentenceTransformer\n","import torch\n","import transformers\n","from transformers import BertTokenizer, BertModel\n","from sklearn.decomposition import PCA\n","from google.colab import drive\n","from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n","import matplotlib.pyplot as plt\n","import os\n","from sklearn.linear_model import LinearRegression\n","import math"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22720,"status":"ok","timestamp":1683644653307,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"},"user_tz":-120},"id":"uQroTSQi56nJ","outputId":"9840a21e-0dce-411c-dc76-5487fb151e95"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["ISDRIVE=False\n","path = \"\"\n","if ISDRIVE:\n","    drive.mount('/content/drive', force_remount=True)\n","    path = \"/content/drive/MyDrive/Master Thesis/\"\n","    \n","log_path = path + \"Logs/\"\n","splits_path = path+\"splits/\"\n","reference_space_path = path + \"Reference spaces/\"\n","pca_embeddings_path = path + \"Generation of embeddings + experiments/Data/PCA embeddings/\"\n","cache_path = path+\"cache/\"\n","splitter = \"; \"\n","splitter2 = \": \"\n","\n","if torch.cuda.is_available():\n","  device = torch.device(\"cuda\")\n","else:\n","  device = torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1683644653308,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"},"user_tz":-120},"id":"4IqNGYcMF59L"},"outputs":[],"source":["def reset_log(log_filename):\n","  with open(log_filename, \"w\") as f:\n","    f.write(\"\")\n","\n","def write_line(obj, log_filename):\n","  s = \"\"\n","  counter = 0\n","  n = len(obj.keys())\n","  for key in obj:\n","    counter += 1\n","    s += f\"{key}{splitter2}{obj[key]}\"\n","    if counter != n:\n","      s += splitter\n","  s += \"\\n\"\n","  with open(log_filename, 'a') as f:\n","    f.write(s)\n","\n","def exists_in_log(models, ks, log_filename):\n","  with open(log_filename, \"r\") as f:\n","    content = f.readlines()\n","  models_target = set()\n","  ks_target = set()\n","  for line in content:\n","    line = line.replace(\"\\n\", \"\").split(splitter)\n","    assert(len(line) == 4)\n","    mapper = {}\n","    for elm in line:\n","      elm = elm.split(splitter2)\n","      assert(len(elm) == 2)\n","      key, value = elm[0], elm[1]\n","      mapper[key] = value\n","    assert(\"model\" in mapper and \"k\" in mapper)\n","    if mapper[\"model\"] in models:\n","      models_target.add(mapper[\"model\"])\n","      ks_target.add(int(mapper[\"k\"]))\n","  models = set(models)\n","  ks = set(ks)\n","  return models == models_target and ks == ks_target"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1683644742651,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"},"user_tz":-120},"id":"36RYajg538sg"},"outputs":[],"source":["def load_keys(filename):\n","    with open(splits_path+filename, \"r\") as f:\n","        lines = f.readlines()\n","    keys = list(map(lambda line: line.replace(\"\\n\", \"\"), lines))\n","    return keys\n","\n","def load_lm_embeddings(model, keys, reference_space_key, key_type_key):\n","  embeddings = np.load(pca_embeddings_path + f\"{model}_{reference_space_key}_{key_type_key}_pca_embeddings.pkl\", allow_pickle=True).to_numpy()\n","  rows = []\n","  indexes = []\n","  added = set()\n","  for row in embeddings:\n","    key = row[0]\n","    if \"[CLS]\" in key:\n","      key = key.replace(\"[CLS] \", \"\")\n","    if \"[SEP]\" in key:\n","      key = key.replace(\" [SEP]\", \"\")\n","    if key in keys and key not in added:\n","      if isinstance(row[1], list):\n","        rows.append(np.array(row[1]).astype(float))\n","      else:\n","        rows.append(np.array(row[1].astype(float)))\n","      indexes.append(key)\n","      added.add(key)\n","  rows = np.array(rows)\n","\n","  indexes = np.array(indexes)\n","  df = pd.DataFrame(rows, index=indexes)\n","  return df\n","\n","def load_embeddings(model, filename, reference_space_key, key_type_key, clear_cache=False):\n","  cache_key = filename.replace(\".txt\", f\"{model}_{reference_space_key}_{key_type_key}_embeddings_procrustes_cache.pkl\")\n","  if not clear_cache:\n","    cache = try_load_df_cache(cache_key)\n","    if cache is not None:\n","      return cache\n","\n","  keys = load_keys(filename)\n","  retval = None\n","  if model in [\"BERT-Tiny\", \"BERT-Mini\", \"BERT-Small\", \"BERT-Medium\", \"BERT-Base\", 'pythia-70m', 'pythia-160m', 'pythia-410m', 'pythia-1b', 'pythia-2.8b', 'pythia-6.9b', \"XLNet-base\", \"XLNet-large\", \"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\", \"ada-002\", \"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\", \"opt-125m\", \"opt-350m\", \"opt-1.3b\", \"opt-2.7b\", \"opt-6.7b\"]:\n","    retval = load_lm_embeddings(model, keys, reference_space_key, key_type_key)\n","  if retval is None:\n","    raise Exception(\"Unknown model\")\n","  retval = retval.sort_index()\n","  write_to_df_cache(retval, cache_key)\n","  return retval\n","\n","def try_load_df_cache(filename):\n","  assert(\".pkl\" in filename)\n","  if os.path.exists(cache_path + filename):\n","    return pd.read_pickle(cache_path + filename)\n","  return None\n","\n","def write_to_df_cache(df, filename):\n","  df.to_pickle(cache_path + filename) \n","\n","def load_reference_space(filename, reference_space_key, key_type_key, clear_cache=False):\n","  cache_key = filename.replace(\".txt\", f\"{reference_space_key}_{key_type_key}_reference_space_procrustes_cache.pkl\")\n","  if not clear_cache:\n","    cache = try_load_df_cache(cache_key)\n","    if cache is not None:\n","      return cache\n","\n","  keys = load_keys(filename)\n","  reference_space_filename = f'{reference_space_key}_{key_type_key}_reference_space.npy'\n","  X = np.load(reference_space_path + reference_space_filename, mmap_mode='r')\n","  rows = []\n","  indexes = []\n","  added = set()\n","  n_cols = X.shape[1]\n","  for row in X:\n","    if row[n_cols-1] in keys and row[n_cols-1] not in added:\n","      rows.append(np.array(row[:n_cols-1].astype(float)))\n","      indexes.append(row[n_cols-1])\n","      added.add(row[n_cols-1])\n","\n","  rows = np.array(rows)\n","\n","  indexes = np.array(indexes)\n","  df = pd.DataFrame(rows, index=indexes)\n","  df = df.sort_index()\n","  write_to_df_cache(df, cache_key)\n","  return df\n","\n","def cosine(a,b):\n","  cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n","  return cos_sim\n","\n","def precision_at_k_analysis(source, target, k):\n","  n = source.shape[0]\n","  corrects_cosine = []\n","  corrects_euclidian = []\n","\n","  distances_euclidian = euclidean_distances(source, target)\n","  distances_euclidian = pd.DataFrame(distances_euclidian, index=source.index, columns=source.index)\n","  for key in distances_euclidian.index:\n","    distances = distances_euclidian.loc[key].sort_values(ascending=True)[:k]\n","    corrects_euclidian.append(1 if key in distances.index else 0)\n","\n","  distances_cosine = cosine_similarity(source.values, target.values)\n","  distances_cosine = pd.DataFrame(distances_cosine, index=source.index, columns=source.index)\n","  for key in distances_cosine.index:\n","    distances = distances_cosine.loc[key].sort_values(ascending=False)[:k]\n","    corrects_cosine.append(1 if key in distances.index else 0)\n","\n","  return sum(corrects_cosine) / len(corrects_cosine), sum(corrects_euclidian) / len(corrects_euclidian)\n","\n","def translate(df):\n","  indexes = df.index\n","  arr = df.to_numpy()\n","  arr -= np.mean(arr, 0)\n","  arr /= np.linalg.norm(arr)\n","  df = pd.DataFrame(arr, index = indexes)\n","  return df\n","\n","def transform(df, R, sca):\n","  return df.dot(R) * sca\n","\n","def cosine_test(a,b):\n","  return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"578DlfrR4SIn"},"source":["#Experiments"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1683644744471,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"},"user_tz":-120},"id":"8A3pdgR8E4lf"},"outputs":[],"source":["bert_models = [\"BERT-Tiny\", \"BERT-Mini\", \"BERT-Small\", \"BERT-Medium\", \"BERT-Base\"]\n","pynthia_models = ['pythia-70m', 'pythia-160m', 'pythia-410m', 'pythia-1b', 'pythia-2.8b', 'pythia-6.9b']\n","t5_models = [\"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\"]\n","opt_models = [\"opt-125m\", \"opt-350m\", \"opt-1.3b\", \"opt-2.7b\", \"opt-6.7b\"]\n","gpt_models = ['gpt2','gpt2-medium','gpt2-large', \"gpt2-xl\", \"ada-002\"]\n","XLNet_models = [\"XLNet-base\", \"XLNet-large\"]\n","all_models = [bert_models, opt_models, gpt_models, pynthia_models]\n","#all_models = [pynthia_models]\n","ks = [1,10,20,50]"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1683644744821,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"},"user_tz":-120},"id":"ShozqTZ2ieD1"},"outputs":[],"source":["def run_experiment(models, train_filename, test_filename, reference_space_key, key_type_key, log_filename):\n","  if not exists_in_log(models, ks, log_filename):\n","    print(f\"Experiment for: models: {models}\")\n","    print(\"Loading referfence space\")\n","    clear_cache=True\n","    y_train, y_test = load_reference_space(train_filename, reference_space_key, key_type_key, clear_cache=clear_cache), load_reference_space(test_filename, reference_space_key, key_type_key, clear_cache=clear_cache)\n","\n","    for model in models:\n","      lm_filepath = pca_embeddings_path + f\"{model}_{reference_space_key}_{key_type_key}_pca_embeddings.pkl\"\n","      if not os.path.exists(lm_filepath):\n","        print(f\"Skipping {model}\")\n","        continue\n","\n","      print(f\"Running model: {model}\")\n","      print(\"Loading embeddings\")\n","      X_train, X_test = load_embeddings(model, train_filename, reference_space_key, key_type_key, clear_cache=clear_cache), load_embeddings(model, test_filename, reference_space_key, key_type_key, clear_cache=clear_cache)\n","      print(f\"Train size: {len(X_train.index.to_list())}, test_size: {len(X_test.index.to_list())}\")\n","      assert(len(set(X_train.index.to_list())) == len(X_train.index.to_list()))\n","      assert(len(set(X_test.index.to_list())) == len(X_test.index.to_list()))\n","      \n","      print(\"Doing procrustes\")\n","      #print(X_train.shape, y_train.shape)\n","      translate(y_train)            \n","      translate(X_train)\n","      print(X_train.shape, y_train.shape)\n","      R, sca = orthogonal_procrustes(X_train, y_train)\n","\n","      translate(y_test)\n","      translate(X_test)\n","      y_test_pred = transform(X_test, R, sca)\n","\n","      print(\"Evaluating\")\n","      for k in ks:\n","        test_acc_cosine, test_acc_euclidian = precision_at_k_analysis(y_test_pred, y_test, k)\n","        write_line({\"model\": model, \"k\": k, \"test P@K cosine\": test_acc_cosine, \"test P@K euclidian\": test_acc_euclidian}, log_filename)\n","  else:\n","    print(f\"Already exists in log: models: {reference_space_key}, {key_type_key}, {models}\")"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1683644746537,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"},"user_tz":-120},"id":"NYiFl0AgFAAH"},"outputs":[],"source":["def run_all(reference_space_key, key_type_key, append=False):\n","  log_filename = log_path + f\"log_procrustes_{reference_space_key}_{key_type_key}.txt\"\n","  test_filename = f\"test_{reference_space_key}_{key_type_key}.txt\"\n","  train_filename = f\"train_{reference_space_key}_{key_type_key}.txt\"\n","\n","  if not append and os.path.exists(log_filename):\n","    print(f\"Experiment data exists: {reference_space_key}, {key_type_key}\")\n","  else:\n","    if not append:\n","      reset_log(log_filename)\n","    for models in all_models:\n","      run_experiment(models, train_filename, test_filename, reference_space_key, key_type_key, log_filename)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":817530,"status":"ok","timestamp":1683645565808,"user":{"displayName":"Mathias Gammelgaard","userId":"00674963895816813210"},"user_tz":-120},"id":"G0QJXKcjpDdu","outputId":"e71089bc-595b-4ef0-9cf0-97f166c119fe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Experiment for: models: ['BERT-Tiny', 'BERT-Mini', 'BERT-Small', 'BERT-Medium', 'BERT-Base']\n","Loading referfence space\n","Skipping BERT-Tiny\n","Running model: BERT-Mini\n","Loading embeddings\n","Train size: 11711, test_size: 2927\n","Doing procrustes\n","(11711, 200) (11711, 200)\n","Evaluating\n","Running model: BERT-Small\n","Loading embeddings\n","Train size: 11711, test_size: 2927\n","Doing procrustes\n","(11711, 200) (11711, 200)\n","Evaluating\n","Running model: BERT-Medium\n","Loading embeddings\n","Train size: 11711, test_size: 2927\n","Doing procrustes\n","(11711, 200) (11711, 200)\n","Evaluating\n","Running model: BERT-Base\n","Loading embeddings\n","Train size: 11711, test_size: 2927\n","Doing procrustes\n","(11711, 200) (11711, 200)\n","Evaluating\n","Already exists in log: models: biggraph, 20K, ['opt-125m', 'opt-350m', 'opt-1.3b', 'opt-2.7b', 'opt-6.7b']\n","Already exists in log: models: biggraph, 20K, ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'ada-002']\n","Already exists in log: models: biggraph, 20K, ['pythia-70m', 'pythia-160m', 'pythia-410m', 'pythia-1b', 'pythia-2.8b', 'pythia-6.9b']\n","Experiment for: models: ['BERT-Tiny', 'BERT-Mini', 'BERT-Small', 'BERT-Medium', 'BERT-Base']\n","Loading referfence space\n","Skipping BERT-Tiny\n","Running model: BERT-Mini\n","Loading embeddings\n","Train size: 11602, test_size: 2900\n","Doing procrustes\n","(11602, 200) (11602, 200)\n","Evaluating\n","Running model: BERT-Small\n","Loading embeddings\n","Train size: 11602, test_size: 2900\n","Doing procrustes\n","(11602, 200) (11602, 200)\n","Evaluating\n","Running model: BERT-Medium\n","Loading embeddings\n","Train size: 11602, test_size: 2900\n","Doing procrustes\n","(11602, 200) (11602, 200)\n","Evaluating\n","Running model: BERT-Base\n","Loading embeddings\n","Train size: 11602, test_size: 2900\n","Doing procrustes\n","(11602, 200) (11602, 200)\n","Evaluating\n","Already exists in log: models: biggraph, places, ['opt-125m', 'opt-350m', 'opt-1.3b', 'opt-2.7b', 'opt-6.7b']\n","Already exists in log: models: biggraph, places, ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'ada-002']\n","Already exists in log: models: biggraph, places, ['pythia-70m', 'pythia-160m', 'pythia-410m', 'pythia-1b', 'pythia-2.8b', 'pythia-6.9b']\n","Experiment for: models: ['BERT-Tiny', 'BERT-Mini', 'BERT-Small', 'BERT-Medium', 'BERT-Base']\n","Loading referfence space\n","Skipping BERT-Tiny\n","Running model: BERT-Mini\n","Loading embeddings\n","Train size: 7296, test_size: 1824\n","Doing procrustes\n","(7296, 200) (7296, 200)\n","Evaluating\n","Running model: BERT-Small\n","Loading embeddings\n","Train size: 7296, test_size: 1824\n","Doing procrustes\n","(7296, 200) (7296, 200)\n","Evaluating\n","Running model: BERT-Medium\n","Loading embeddings\n","Train size: 7296, test_size: 1824\n","Doing procrustes\n","(7296, 200) (7296, 200)\n","Evaluating\n","Running model: BERT-Base\n","Loading embeddings\n","Train size: 7296, test_size: 1824\n","Doing procrustes\n","(7296, 200) (7296, 200)\n","Evaluating\n","Already exists in log: models: biggraph, names, ['opt-125m', 'opt-350m', 'opt-1.3b', 'opt-2.7b', 'opt-6.7b']\n","Already exists in log: models: biggraph, names, ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'ada-002']\n","Already exists in log: models: biggraph, names, ['pythia-70m', 'pythia-160m', 'pythia-410m', 'pythia-1b', 'pythia-2.8b', 'pythia-6.9b']\n","Experiment for: models: ['BERT-Tiny', 'BERT-Mini', 'BERT-Small', 'BERT-Medium', 'BERT-Base']\n","Loading referfence space\n","Skipping BERT-Tiny\n","Running model: BERT-Mini\n","Loading embeddings\n","Train size: 1600, test_size: 400\n","Doing procrustes\n","(1600, 200) (1600, 200)\n","Evaluating\n","Running model: BERT-Small\n","Loading embeddings\n","Train size: 1600, test_size: 400\n","Doing procrustes\n","(1600, 200) (1600, 200)\n","Evaluating\n","Running model: BERT-Medium\n","Loading embeddings\n","Train size: 1600, test_size: 400\n","Doing procrustes\n","(1600, 200) (1600, 200)\n","Evaluating\n","Running model: BERT-Base\n","Loading embeddings\n","Train size: 1600, test_size: 400\n","Doing procrustes\n","(1600, 200) (1600, 200)\n","Evaluating\n","Already exists in log: models: biggraph, 20K_1_to_1_synsets, ['opt-125m', 'opt-350m', 'opt-1.3b', 'opt-2.7b', 'opt-6.7b']\n","Already exists in log: models: biggraph, 20K_1_to_1_synsets, ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'ada-002']\n","Already exists in log: models: biggraph, 20K_1_to_1_synsets, ['pythia-70m', 'pythia-160m', 'pythia-410m', 'pythia-1b', 'pythia-2.8b', 'pythia-6.9b']\n","Experiment for: models: ['BERT-Tiny', 'BERT-Mini', 'BERT-Small', 'BERT-Medium', 'BERT-Base']\n","Loading referfence space\n","Skipping BERT-Tiny\n","Running model: BERT-Mini\n","Loading embeddings\n","Train size: 1600, test_size: 400\n","Doing procrustes\n","(1600, 200) (1600, 200)\n","Evaluating\n","Running model: BERT-Small\n","Loading embeddings\n","Train size: 1600, test_size: 400\n","Doing procrustes\n","(1600, 200) (1600, 200)\n","Evaluating\n","Running model: BERT-Medium\n","Loading embeddings\n","Train size: 1600, test_size: 400\n","Doing procrustes\n","(1600, 200) (1600, 200)\n","Evaluating\n","Running model: BERT-Base\n","Loading embeddings\n","Train size: 1600, test_size: 400\n","Doing procrustes\n","(1600, 200) (1600, 200)\n","Evaluating\n","Already exists in log: models: biggraph, 20K_2_to_3_synsets, ['opt-125m', 'opt-350m', 'opt-1.3b', 'opt-2.7b', 'opt-6.7b']\n","Already exists in log: models: biggraph, 20K_2_to_3_synsets, ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'ada-002']\n","Already exists in log: models: biggraph, 20K_2_to_3_synsets, ['pythia-70m', 'pythia-160m', 'pythia-410m', 'pythia-1b', 'pythia-2.8b', 'pythia-6.9b']\n","Experiment for: models: ['BERT-Tiny', 'BERT-Mini', 'BERT-Small', 'BERT-Medium', 'BERT-Base']\n","Loading referfence space\n","Skipping BERT-Tiny\n","Running model: BERT-Mini\n","Loading embeddings\n","Train size: 1600, test_size: 400\n","Doing procrustes\n","(1600, 200) (1600, 200)\n","Evaluating\n","Running model: BERT-Small\n","Loading embeddings\n","Train size: 1600, test_size: 400\n","Doing procrustes\n","(1600, 200) (1600, 200)\n","Evaluating\n","Running model: BERT-Medium\n","Loading embeddings\n","Train size: 1600, test_size: 400\n","Doing procrustes\n","(1600, 200) (1600, 200)\n","Evaluating\n","Running model: BERT-Base\n","Loading embeddings\n","Train size: 1600, test_size: 400\n","Doing procrustes\n","(1600, 200) (1600, 200)\n","Evaluating\n","Already exists in log: models: biggraph, 20K_4_to_infinity_synsets, ['opt-125m', 'opt-350m', 'opt-1.3b', 'opt-2.7b', 'opt-6.7b']\n","Already exists in log: models: biggraph, 20K_4_to_infinity_synsets, ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'ada-002']\n","Already exists in log: models: biggraph, 20K_4_to_infinity_synsets, ['pythia-70m', 'pythia-160m', 'pythia-410m', 'pythia-1b', 'pythia-2.8b', 'pythia-6.9b']\n","Experiment for: models: ['BERT-Tiny', 'BERT-Mini', 'BERT-Small', 'BERT-Medium', 'BERT-Base']\n","Loading referfence space\n","Skipping BERT-Tiny\n","Skipping BERT-Mini\n","Running model: BERT-Small\n","Loading embeddings\n","Train size: 8792, test_size: 2198\n","Doing procrustes\n","(8792, 512) (8792, 512)\n","Evaluating\n","Running model: BERT-Medium\n","Loading embeddings\n","Train size: 8792, test_size: 2198\n","Doing procrustes\n","(8792, 512) (8792, 512)\n","Evaluating\n","Running model: BERT-Base\n","Loading embeddings\n","Train size: 8792, test_size: 2198\n","Doing procrustes\n","(8792, 512) (8792, 512)\n","Evaluating\n","Already exists in log: models: transe, 20K, ['opt-125m', 'opt-350m', 'opt-1.3b', 'opt-2.7b', 'opt-6.7b']\n","Already exists in log: models: transe, 20K, ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'ada-002']\n","Already exists in log: models: transe, 20K, ['pythia-70m', 'pythia-160m', 'pythia-410m', 'pythia-1b', 'pythia-2.8b', 'pythia-6.9b']\n","Experiment for: models: ['BERT-Tiny', 'BERT-Mini', 'BERT-Small', 'BERT-Medium', 'BERT-Base']\n","Loading referfence space\n","Skipping BERT-Tiny\n","Skipping BERT-Mini\n","Running model: BERT-Small\n","Loading embeddings\n","Train size: 13432, test_size: 3358\n","Doing procrustes\n","(13432, 512) (13432, 512)\n","Evaluating\n","Running model: BERT-Medium\n","Loading embeddings\n","Train size: 13432, test_size: 3358\n","Doing procrustes\n","(13432, 512) (13432, 512)\n","Evaluating\n","Running model: BERT-Base\n","Loading embeddings\n","Train size: 13432, test_size: 3358\n","Doing procrustes\n","(13432, 512) (13432, 512)\n","Evaluating\n","Already exists in log: models: transe, places, ['opt-125m', 'opt-350m', 'opt-1.3b', 'opt-2.7b', 'opt-6.7b']\n","Already exists in log: models: transe, places, ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'ada-002']\n","Already exists in log: models: transe, places, ['pythia-70m', 'pythia-160m', 'pythia-410m', 'pythia-1b', 'pythia-2.8b', 'pythia-6.9b']\n","Experiment for: models: ['BERT-Tiny', 'BERT-Mini', 'BERT-Small', 'BERT-Medium', 'BERT-Base']\n","Loading referfence space\n","Skipping BERT-Tiny\n","Skipping BERT-Mini\n","Running model: BERT-Small\n","Loading embeddings\n","Train size: 5072, test_size: 1268\n","Doing procrustes\n","(5072, 512) (5072, 512)\n","Evaluating\n","Running model: BERT-Medium\n","Loading embeddings\n","Train size: 5072, test_size: 1268\n","Doing procrustes\n","(5072, 512) (5072, 512)\n","Evaluating\n","Running model: BERT-Base\n","Loading embeddings\n","Train size: 5072, test_size: 1268\n","Doing procrustes\n","(5072, 512) (5072, 512)\n","Evaluating\n","Already exists in log: models: transe, names, ['opt-125m', 'opt-350m', 'opt-1.3b', 'opt-2.7b', 'opt-6.7b']\n","Already exists in log: models: transe, names, ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'ada-002']\n","Already exists in log: models: transe, names, ['pythia-70m', 'pythia-160m', 'pythia-410m', 'pythia-1b', 'pythia-2.8b', 'pythia-6.9b']\n","Experiment for: models: ['BERT-Tiny', 'BERT-Mini', 'BERT-Small', 'BERT-Medium', 'BERT-Base']\n","Loading referfence space\n","Skipping BERT-Tiny\n","Skipping BERT-Mini\n","Running model: BERT-Small\n","Loading embeddings\n","Train size: 1600, test_size: 400\n","Doing procrustes\n","(1600, 512) (1600, 512)\n","Evaluating\n","Running model: BERT-Medium\n","Loading embeddings\n","Train size: 1600, test_size: 400\n","Doing procrustes\n","(1600, 512) (1600, 512)\n","Evaluating\n","Running model: BERT-Base\n","Loading embeddings\n","Train size: 1600, test_size: 400\n","Doing procrustes\n","(1600, 512) (1600, 512)\n","Evaluating\n","Already exists in log: models: transe, 20K_1_to_1_synsets, ['opt-125m', 'opt-350m', 'opt-1.3b', 'opt-2.7b', 'opt-6.7b']\n","Already exists in log: models: transe, 20K_1_to_1_synsets, ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'ada-002']\n","Already exists in log: models: transe, 20K_1_to_1_synsets, ['pythia-70m', 'pythia-160m', 'pythia-410m', 'pythia-1b', 'pythia-2.8b', 'pythia-6.9b']\n","Experiment for: models: ['BERT-Tiny', 'BERT-Mini', 'BERT-Small', 'BERT-Medium', 'BERT-Base']\n","Loading referfence space\n","Skipping BERT-Tiny\n","Skipping BERT-Mini\n","Running model: BERT-Small\n","Loading embeddings\n","Train size: 1600, test_size: 400\n","Doing procrustes\n","(1600, 512) (1600, 512)\n","Evaluating\n","Running model: BERT-Medium\n","Loading embeddings\n","Train size: 1600, test_size: 400\n","Doing procrustes\n","(1600, 512) (1600, 512)\n","Evaluating\n","Running model: BERT-Base\n","Loading embeddings\n","Train size: 1600, test_size: 400\n","Doing procrustes\n","(1600, 512) (1600, 512)\n","Evaluating\n","Already exists in log: models: transe, 20K_2_to_3_synsets, ['opt-125m', 'opt-350m', 'opt-1.3b', 'opt-2.7b', 'opt-6.7b']\n","Already exists in log: models: transe, 20K_2_to_3_synsets, ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'ada-002']\n","Already exists in log: models: transe, 20K_2_to_3_synsets, ['pythia-70m', 'pythia-160m', 'pythia-410m', 'pythia-1b', 'pythia-2.8b', 'pythia-6.9b']\n","Experiment for: models: ['BERT-Tiny', 'BERT-Mini', 'BERT-Small', 'BERT-Medium', 'BERT-Base']\n","Loading referfence space\n","Skipping BERT-Tiny\n","Skipping BERT-Mini\n","Running model: BERT-Small\n","Loading embeddings\n","Train size: 1600, test_size: 400\n","Doing procrustes\n","(1600, 512) (1600, 512)\n","Evaluating\n","Running model: BERT-Medium\n","Loading embeddings\n","Train size: 1600, test_size: 400\n","Doing procrustes\n","(1600, 512) (1600, 512)\n","Evaluating\n","Running model: BERT-Base\n","Loading embeddings\n","Train size: 1600, test_size: 400\n","Doing procrustes\n","(1600, 512) (1600, 512)\n","Evaluating\n","Already exists in log: models: transe, 20K_4_to_infinity_synsets, ['opt-125m', 'opt-350m', 'opt-1.3b', 'opt-2.7b', 'opt-6.7b']\n","Already exists in log: models: transe, 20K_4_to_infinity_synsets, ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'ada-002']\n","Already exists in log: models: transe, 20K_4_to_infinity_synsets, ['pythia-70m', 'pythia-160m', 'pythia-410m', 'pythia-1b', 'pythia-2.8b', 'pythia-6.9b']\n","Experiment for: models: ['BERT-Tiny', 'BERT-Mini', 'BERT-Small', 'BERT-Medium', 'BERT-Base']\n","Loading referfence space\n","Skipping BERT-Tiny\n","Skipping BERT-Mini\n","Running model: BERT-Small\n","Loading embeddings\n","Train size: 8792, test_size: 2198\n","Doing procrustes\n","(8792, 512) (8792, 512)\n","Evaluating\n","Running model: BERT-Medium\n","Loading embeddings\n","Train size: 8792, test_size: 2198\n","Doing procrustes\n","(8792, 512) (8792, 512)\n","Evaluating\n","Running model: BERT-Base\n","Loading embeddings\n","Train size: 8792, test_size: 2198\n","Doing procrustes\n","(8792, 512) (8792, 512)\n","Evaluating\n","Already exists in log: models: complex, 20K, ['opt-125m', 'opt-350m', 'opt-1.3b', 'opt-2.7b', 'opt-6.7b']\n","Already exists in log: models: complex, 20K, ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'ada-002']\n","Already exists in log: models: complex, 20K, ['pythia-70m', 'pythia-160m', 'pythia-410m', 'pythia-1b', 'pythia-2.8b', 'pythia-6.9b']\n","Experiment for: models: ['BERT-Tiny', 'BERT-Mini', 'BERT-Small', 'BERT-Medium', 'BERT-Base']\n","Loading referfence space\n","Skipping BERT-Tiny\n","Skipping BERT-Mini\n","Running model: BERT-Small\n","Loading embeddings\n","Train size: 13432, test_size: 3358\n","Doing procrustes\n","(13432, 512) (13432, 512)\n","Evaluating\n","Running model: BERT-Medium\n","Loading embeddings\n","Train size: 13432, test_size: 3358\n","Doing procrustes\n","(13432, 512) (13432, 512)\n","Evaluating\n","Running model: BERT-Base\n","Loading embeddings\n","Train size: 13432, test_size: 3358\n","Doing procrustes\n","(13432, 512) (13432, 512)\n","Evaluating\n","Already exists in log: models: complex, places, ['opt-125m', 'opt-350m', 'opt-1.3b', 'opt-2.7b', 'opt-6.7b']\n","Already exists in log: models: complex, places, ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'ada-002']\n","Already exists in log: models: complex, places, ['pythia-70m', 'pythia-160m', 'pythia-410m', 'pythia-1b', 'pythia-2.8b', 'pythia-6.9b']\n","Experiment for: models: ['BERT-Tiny', 'BERT-Mini', 'BERT-Small', 'BERT-Medium', 'BERT-Base']\n","Loading referfence space\n","Skipping BERT-Tiny\n","Skipping BERT-Mini\n","Running model: BERT-Small\n","Loading embeddings\n","Train size: 5072, test_size: 1268\n","Doing procrustes\n","(5072, 512) (5072, 512)\n","Evaluating\n","Running model: BERT-Medium\n","Loading embeddings\n","Train size: 5072, test_size: 1268\n","Doing procrustes\n","(5072, 512) (5072, 512)\n","Evaluating\n","Running model: BERT-Base\n","Loading embeddings\n","Train size: 5072, test_size: 1268\n","Doing procrustes\n","(5072, 512) (5072, 512)\n","Evaluating\n","Already exists in log: models: complex, names, ['opt-125m', 'opt-350m', 'opt-1.3b', 'opt-2.7b', 'opt-6.7b']\n","Already exists in log: models: complex, names, ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'ada-002']\n","Already exists in log: models: complex, names, ['pythia-70m', 'pythia-160m', 'pythia-410m', 'pythia-1b', 'pythia-2.8b', 'pythia-6.9b']\n","Experiment for: models: ['BERT-Tiny', 'BERT-Mini', 'BERT-Small', 'BERT-Medium', 'BERT-Base']\n","Loading referfence space\n","Skipping BERT-Tiny\n","Skipping BERT-Mini\n","Running model: BERT-Small\n","Loading embeddings\n","Train size: 1600, test_size: 400\n","Doing procrustes\n","(1600, 512) (1600, 512)\n","Evaluating\n","Running model: BERT-Medium\n","Loading embeddings\n","Train size: 1600, test_size: 400\n","Doing procrustes\n","(1600, 512) (1600, 512)\n","Evaluating\n","Running model: BERT-Base\n","Loading embeddings\n","Train size: 1600, test_size: 400\n","Doing procrustes\n","(1600, 512) (1600, 512)\n","Evaluating\n","Already exists in log: models: complex, 20K_1_to_1_synsets, ['opt-125m', 'opt-350m', 'opt-1.3b', 'opt-2.7b', 'opt-6.7b']\n","Already exists in log: models: complex, 20K_1_to_1_synsets, ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'ada-002']\n","Already exists in log: models: complex, 20K_1_to_1_synsets, ['pythia-70m', 'pythia-160m', 'pythia-410m', 'pythia-1b', 'pythia-2.8b', 'pythia-6.9b']\n","Experiment for: models: ['BERT-Tiny', 'BERT-Mini', 'BERT-Small', 'BERT-Medium', 'BERT-Base']\n","Loading referfence space\n","Skipping BERT-Tiny\n","Skipping BERT-Mini\n","Running model: BERT-Small\n","Loading embeddings\n","Train size: 1600, test_size: 400\n","Doing procrustes\n","(1600, 512) (1600, 512)\n","Evaluating\n","Running model: BERT-Medium\n","Loading embeddings\n","Train size: 1600, test_size: 400\n","Doing procrustes\n","(1600, 512) (1600, 512)\n","Evaluating\n","Running model: BERT-Base\n","Loading embeddings\n","Train size: 1600, test_size: 400\n","Doing procrustes\n","(1600, 512) (1600, 512)\n","Evaluating\n","Already exists in log: models: complex, 20K_2_to_3_synsets, ['opt-125m', 'opt-350m', 'opt-1.3b', 'opt-2.7b', 'opt-6.7b']\n","Already exists in log: models: complex, 20K_2_to_3_synsets, ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'ada-002']\n","Already exists in log: models: complex, 20K_2_to_3_synsets, ['pythia-70m', 'pythia-160m', 'pythia-410m', 'pythia-1b', 'pythia-2.8b', 'pythia-6.9b']\n","Experiment for: models: ['BERT-Tiny', 'BERT-Mini', 'BERT-Small', 'BERT-Medium', 'BERT-Base']\n","Loading referfence space\n","Skipping BERT-Tiny\n","Skipping BERT-Mini\n","Running model: BERT-Small\n","Loading embeddings\n","Train size: 1600, test_size: 400\n","Doing procrustes\n","(1600, 512) (1600, 512)\n","Evaluating\n","Running model: BERT-Medium\n","Loading embeddings\n","Train size: 1600, test_size: 400\n","Doing procrustes\n","(1600, 512) (1600, 512)\n","Evaluating\n","Running model: BERT-Base\n","Loading embeddings\n","Train size: 1600, test_size: 400\n","Doing procrustes\n","(1600, 512) (1600, 512)\n","Evaluating\n","Already exists in log: models: complex, 20K_4_to_infinity_synsets, ['opt-125m', 'opt-350m', 'opt-1.3b', 'opt-2.7b', 'opt-6.7b']\n","Already exists in log: models: complex, 20K_4_to_infinity_synsets, ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'ada-002']\n","Already exists in log: models: complex, 20K_4_to_infinity_synsets, ['pythia-70m', 'pythia-160m', 'pythia-410m', 'pythia-1b', 'pythia-2.8b', 'pythia-6.9b']\n"]}],"source":["reference_spaces = [\"biggraph\", \"transe\", \"complex\"]\n","key_type_keys = [\"20K\", \"places\", \"names\", \"20K_1_to_1_synsets\", \"20K_2_to_3_synsets\", \"20K_4_to_infinity_synsets\"]\n","\n","for reference_space in reference_spaces:\n","  for key_type_key in key_type_keys:\n","    try:\n","      run_all(reference_space, key_type_key, append=True)\n","    except Exception as e:\n","      print(key_type_key, reference_space)\n","      raise e"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uA0nz0agxaH3"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
